{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_regression_grad(X, t, w, delta):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the average Huber (NOT squared error) loss for robust regression.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        X: numpy array\n",
    "            N x (D+1) numpy array for the train inputs (with dummy variables)\n",
    "        t: numpy array\n",
    "            N x 1 numpy array for the train targets\n",
    "        w: numpy array\n",
    "            (D+1) x 1 numpy array for the weights\n",
    "        delta: positive float\n",
    "            parameter for huber loss.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        dw: numpy array\n",
    "            (D+1) x 1 numpy array, the gradient of the huber loss in w\n",
    "    \"\"\"\n",
    "    \n",
    "    # Number of training examples\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    # Compute the residuals (r = Xw - t)\n",
    "    residuals = X @ w - t\n",
    "    \n",
    "    # Initialize the gradient\n",
    "    dw = np.zeros_like(w)\n",
    "    \n",
    "    # Iterate through each training example\n",
    "    for i in range(N):\n",
    "        r = residuals[i]\n",
    "        \n",
    "        if np.abs(r) <= delta:\n",
    "            # Quadratic region: gradient is r * X[i]\n",
    "            dw += r * X[i].reshape(-1, 1)\n",
    "        else:\n",
    "            # Linear region: gradient is delta * sign(r) * X[i]\n",
    "            dw += delta * np.sign(r) * X[i].reshape(-1, 1)\n",
    "    \n",
    "    # Average the gradient over all training examples\n",
    "    dw /= N\n",
    "    \n",
    "    return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization(X, t, delta, lr, num_iterations=10000):\n",
    "    \"\"\"\n",
    "    Compute (nearly) optimal weights for robust linear regression.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        X: numpy array\n",
    "            N x (D+1) numpy array for the train inputs (with dummy variables)\n",
    "        t: numpy array\n",
    "            N x 1 numpy array for the train targets\n",
    "        delta: positive float\n",
    "            parameter for huber loss.\n",
    "        lr: positive float\n",
    "            learning rate or step-size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        w: numpy array\n",
    "            (D+1) x 1 numpy array, (nearly) optimal weights for robust linear regression\n",
    "    \"\"\"\n",
    "\n",
    "    # some initialization for robust regression parameters\n",
    "    w = np.zeros((X.shape[1], 1))\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Calculate predictions\n",
    "        predictions = X @ w\n",
    "        \n",
    "        # Calculate residuals\n",
    "        residuals = predictions - t\n",
    "        \n",
    "        # Compute the gradient of the Huber loss\n",
    "        dw = robust_regression_grad(X, t, w, delta)\n",
    "        \n",
    "        # Update weights\n",
    "        w -= lr * dw\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_error(y, t):\n",
    "    \"\"\"\n",
    "    Compute the average squared error (NOT Huber) loss:\n",
    "    \n",
    "    sum_{i=1}^N (y^i - t^i)^2 / N\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        y: numpy array\n",
    "            N x 1 numpy array for the predictions\n",
    "        t: numpy array\n",
    "            N x 1 numpy array for the train targets\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        cost: float\n",
    "            the average squared error loss\n",
    "    \"\"\"\n",
    "    dif = y - t\n",
    "    L = 0.5 * np.power(dif, 2)\n",
    "    return L.mean()\n",
    "\n",
    "def linear_regression_optimal_weights(X, t):\n",
    "    \"\"\"\n",
    "    Compute the optimal weights for linear regression.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        X: numpy array\n",
    "            N x (D+1) numpy array for the train inputs (with dummy variables)\n",
    "        t: numpy array\n",
    "            N x 1 numpy array for the train targets\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        w: numpy array\n",
    "            (D+1) x 1 numpy array, optimal weights for linear regression\n",
    "    \"\"\"    \n",
    "    w = np.linalg.solve(np.dot(X.T, X), np.dot(X.T, t))\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear regression validation loss: 28.44202896790701\n",
      "delta: 0.1, valid. squared error loss: 4.909228730093657, train squared error loss: 38.04144442529115\n",
      "delta: 0.5, valid. squared error loss: 4.018972761905309, train squared error loss: 38.31116689103212\n",
      "delta: 1, valid. squared error loss: 4.200164017924182, train squared error loss: 37.99343986336346\n",
      "delta: 5, valid. squared error loss: 23.63610501876366, train squared error loss: 28.847567784752847\n",
      "delta: 10, valid. squared error loss: 26.420467729491516, train squared error loss: 27.79829743150903\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "response = requests.get(\"https://www.cs.toronto.edu/~cmaddis/courses/sta314_f21/data/hw2_X.npy\")\n",
    "response.raise_for_status()\n",
    "X = np.load(io.BytesIO(response.content))\n",
    "response = requests.get(\"https://www.cs.toronto.edu/~cmaddis/courses/sta314_f21/data/hw2_t.npy\")\n",
    "response.raise_for_status()\n",
    "t = np.load(io.BytesIO(response.content))\n",
    "t = np.expand_dims(t, 1)\n",
    "(N, D) = X.shape\n",
    "\n",
    "# this code adds the \"dummy variable\"\n",
    "ones_vector = np.ones((N, 1))\n",
    "X = np.concatenate((ones_vector, X), axis=1)\n",
    "\n",
    "# train, validation, test split using numpy indexing\n",
    "X_train, X_val, X_test = X[30:], X[15:30], X[:15]\n",
    "t_train, t_val, t_test = t[30:], t[15:30], t[:15]\n",
    "\n",
    "lr = 0.01  # learning rate\n",
    "\n",
    "# these are the deltas we will try for robust regression\n",
    "deltas = [0.1, 0.5, 1, 5, 10]\n",
    "\n",
    "# Report the validation squared error loss using standard linear regression\n",
    "w_linreg = linear_regression_optimal_weights(X_train, t_train)\n",
    "predictions = np.dot(X_val, w_linreg)\n",
    "val_loss = squared_error(predictions, t_val)\n",
    "print(f\"linear regression validation loss: {val_loss}\")\n",
    "\n",
    "for i, delta in enumerate(deltas):\n",
    "\n",
    "    # Optimize the parameters based on the train data for robust regression.\n",
    "    w = optimization(X_train, t_train, delta, lr)\n",
    "\n",
    "    # Report the validation and training squared error loss for this delta.\n",
    "    val_pred = np.dot(X_val, w)\n",
    "    val_loss = squared_error(t_val, val_pred)\n",
    "    train_pred = np.dot(X_train, w)\n",
    "    train_loss = squared_error(t_train, train_pred)\n",
    "    print(f\"delta: {delta}, valid. squared error loss: {val_loss}, train squared error loss: {train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
